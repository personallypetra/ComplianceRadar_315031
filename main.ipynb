{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compliance Radar – Main Notebook\n",
    "\n",
    "Machine Learning 2025/2026 – LUISS Guido Carli\n",
    "\n",
    "This notebook follows the project structure required by the course:\n",
    "1. Data loading\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Preprocessing & feature engineering\n",
    "4. Model training & cross-validation\n",
    "5. Evaluation & interpretability\n",
    "6. Conclusions & insights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup & Imports\n",
    "\n",
    "Run this cell first to load all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import shap\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "We load the SQLite database `org_compliance_data.db` from the `data/` folder and inspect the available tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"data/org_compliance_data.db\"  # make sure the file is in this path\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "\n",
    "# List tables in the database (syntax works with older SQLAlchemy; for newer you can use inspector)\n",
    "tables = engine.table_names()\n",
    "print(\"Tables in database:\", tables)\n",
    "\n",
    "# EXAMPLE: load a specific table (replace 'table_name' with an actual name from the list above)\n",
    "# df = pd.read_sql(\"SELECT * FROM table_name\", engine)\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "_Teammate A:_ add histograms, boxplots, counts, and correlation heatmaps here once `df` is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example EDA template (uncomment once df is defined)\n",
    "# display(df.head())\n",
    "# display(df.describe(include=\"all\"))\n",
    "\n",
    "# numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "# df[numeric_cols].hist(bins=30, figsize=(12, 8))\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# corr = df[numeric_cols].corr()\n",
    "# sns.heatmap(corr, annot=False, cmap=\"coolwarm\")\n",
    "# plt.title(\"Correlation heatmap\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Feature Engineering\n",
    "\n",
    "Here we define:\n",
    "- target variable `y`\n",
    "- feature matrix `X`\n",
    "- train/test split\n",
    "- scaling/imputation if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example template – adapt based on your actual columns\n",
    "# target_col = \"<TARGET_COLUMN_NAME>\"  # TODO: replace with real target name\n",
    "# feature_cols = [c for c in df.columns if c != target_col]\n",
    "\n",
    "# X = df[feature_cols]\n",
    "# y = df[target_col]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42, stratify=y\n",
    "# )\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training & Cross-Validation\n",
    "\n",
    "Here we train at least 3 models: Logistic Regression, Random Forest, XGBoost.\n",
    "\n",
    "_You (Petra) handle this part._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example placeholder – replace with real code once X_train_scaled / y_train exist\n",
    "\n",
    "# log_reg = LogisticRegression(max_iter=1000)\n",
    "# rf = RandomForestClassifier(random_state=42)\n",
    "# xgb = XGBClassifier(random_state=42, eval_metric=\"logloss\")\n",
    "\n",
    "# models = {\n",
    "#     \"Logistic Regression\": log_reg,\n",
    "#     \"Random Forest\": rf,\n",
    "#     \"XGBoost\": xgb\n",
    "# }\n",
    "\n",
    "# results = []\n",
    "# for name, model in models.items():\n",
    "#     model.fit(X_train_scaled, y_train)\n",
    "#     y_pred = model.predict(X_test_scaled)\n",
    "#     y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "#     acc = accuracy_score(y_test, y_pred)\n",
    "#     prec = precision_score(y_test, y_pred)\n",
    "#     rec = recall_score(y_test, y_pred)\n",
    "#     f1 = f1_score(y_test, y_pred)\n",
    "#     auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "#     results.append([name, acc, prec, rec, f1, auc])\n",
    "\n",
    "# results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUC\"])\n",
    "# display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpretability (Feature Importance & SHAP)\n",
    "\n",
    "_You (Petra) also handle this part._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example placeholder – compute SHAP values for the best model\n",
    "# best_model = rf  # or xgb, depending on results\n",
    "\n",
    "# explainer = shap.TreeExplainer(best_model)\n",
    "# shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "# shap.summary_plot(shap_values, X_test, feature_names=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions & Compliance Insights\n",
    "\n",
    "_Teammate B/C:_ add textual interpretation here based on the final results.\n",
    "\n",
    "- Which features are most strongly associated with potential non-compliance?\n",
    "- How should the organisation monitor these?\n",
    "- What recommendations follow from the model outputs?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
